# A separate file for mac builds, due to different installation runs, absence of tools (code coverage, mold, etc.)
# and inability to specify the _same_ job to run the same steps both for "ubuntu + some image" and "macos + no image" set ups.
#
# Keep its rustc versions the same as in the main testing pipeline file.
name: Build and Test (macOs)

on: push

jobs:
  regression-check:
    strategy:
      matrix:
        rust_toolchain: [1.56]
        build_type: [debug, release]
    timeout-minutes: 30
    name: run regression test suite (macOs)
    runs-on: macos-latest

    defaults:
      run:
        shell: bash
    env:
      RUST_BACKTRACE: 1
      CARGO_INCREMENTAL: 0
      BUILD_TYPE: ${{ matrix.build_type }}
      COPT: '-Werror'

    steps:
      - name: Checkout
        uses: actions/checkout@v3
        with:
          submodules: true
          fetch-depth: 2

      - name: Install rust toolchain ${{ matrix.rust_toolchain }}
        uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ matrix.rust_toolchain }}
          profile: minimal
          override: true
          components: rustfmt, clippy

      - name: Install system dependencies
        run: |
          brew install flex bison poetry jq python@3.9 coreutils

      - name: Set pg revision for caching
        id: pg_ver
        run: echo ::set-output name=pg_rev::$(git rev-parse HEAD:vendor/postgres)

      - name: Cache postgres build
        id: cache_pg
        uses: actions/cache@v3
        with:
          path: |
            tmp_install/
          key: v1-${{ runner.os }}-${{ matrix.build_type }}-pg-${{ steps.pg_ver.outputs.pg_rev }}-${{ hashFiles('Makefile') }}

      - name: Build postgres
        if: steps.cache_pg.outputs.cache-hit != 'true'
        run: |
          make postgres -j$(nproc)

      - name: Cache cargo deps
        id: cache_cargo
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: v1-${{ runner.os }}-${{ matrix.build_type }}-cargo-${{ hashFiles('Cargo.lock') }}

      - name: Check Rust code formatting
        run: cargo fmt --all -- --check

      - name: Run clippy
        run: ./run_clippy.sh

      - name: Run cargo build
        run: |
          if [[ $BUILD_TYPE == "debug" ]]; then
            CARGO_FLAGS=
          elif [[ $BUILD_TYPE == "release" ]]; then
            CARGO_FLAGS=--release
          fi
          cargo build $CARGO_FLAGS --workspace --bins --examples --tests

      - name: Run cargo test
        run: |
          if [[ $BUILD_TYPE == "debug" ]]; then
            CARGO_FLAGS=
          elif [[ $BUILD_TYPE == "release" ]]; then
            CARGO_FLAGS=--release
          fi
          cargo test $CARGO_FLAGS

      - name: Install binaries
        run: |
          mkdir -p /tmp/zenith/pg_install/
          cp -a tmp_install/ /tmp/zenith/pg_install/

          binaries=$(
            cargo metadata --format-version=1 --no-deps |
            jq -r '.packages[].targets[] | select(.kind | index("bin")) | .name'
          )

          test_exe_paths=$(
            cargo test --message-format=json --no-run |
            jq -r '.executable | select(. != null)'
          )

          mkdir -p /tmp/zenith/bin/
          mkdir -p /tmp/zenith/test_bin/
          mkdir -p /tmp/zenith/etc/

          # Install target binaries
          for bin in $binaries; do
            SRC=target/$BUILD_TYPE/$bin
            DST=/tmp/zenith/bin/$bin
            cp $SRC $DST
            echo $DST >> /tmp/zenith/etc/binaries.list
          done

          # Install test executables (for code coverage)
          if [[ $BUILD_TYPE == "debug" ]]; then
            for bin in $test_exe_paths; do
              SRC=$bin
              DST=/tmp/zenith/test_bin/$(basename $bin)
              cp $SRC $DST
              echo $DST >> /tmp/zenith/etc/binaries.list
            done
          fi

      - name: Cache poetry deps
        id: cache_poetry
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pypoetry/virtualenvs
          key: v1-${{ runner.os }}-python-deps-${{ hashFiles('poetry.lock') }}

      - name: Install Python deps
        run: ./scripts/pysync

      - name: Check Python codestyle
        run: |
          poetry run yapf --recursive --diff .
          poetry run mypy .

# TODO kb reuse this from the ubuntu config instead?

      - name: Pytest regress tests
        env:
          ZENITH_BIN: /tmp/zenith/bin/
          POSTGRES_DISTRIB_DIR: /tmp/zenith/pg_install/
          TEST_OUTPUT: /tmp/test_output/
          # this variable will be embedded in perf test report
          # and is needed to distinguish different environments
          PLATFORM: zenith-local-ci
        run: |
          # Run the tests.
          #
          # TODO kb adjust
          # The junit.xml file allows CircleCI to display more fine-grained test information
          # in its "Tests" tab in the results page.
          # --verbose prints name of each test (helpful when there are
          # multiple tests in one file)
          # -rA prints summary in the end
          # -n4 uses four processes to run tests via pytest-xdist
          # -s is not used to prevent pytest from capturing output, because tests are running
          # in parallel and logs are mixed between different tests
          ./scripts/pytest \
            --junitxml=$TEST_OUTPUT/junit.xml \
            --tb=short \
            --verbose \
            -m "not remote_cluster" \
            -rA test_runner/batch_pg_regress -n4

      - name: Pytest other tests
        env:
          ZENITH_BIN: /tmp/zenith/bin/
          POSTGRES_DISTRIB_DIR: /tmp/zenith/pg_install/
          TEST_OUTPUT: /tmp/test_output/
          # this variable will be embedded in perf test report
          # and is needed to distinguish different environments
          PLATFORM: zenith-local-ci
        run: |
          # Run the tests.
          #
          # TODO kb adjust
          # The junit.xml file allows CircleCI to display more fine-grained test information
          # in its "Tests" tab in the results page.
          # --verbose prints name of each test (helpful when there are
          # multiple tests in one file)
          # -rA prints summary in the end
          # -n4 uses four processes to run tests via pytest-xdist
          # -s is not used to prevent pytest from capturing output, because tests are running
          # in parallel and logs are mixed between different tests
          ./scripts/pytest \
            --junitxml=$TEST_OUTPUT/junit.xml \
            --tb=short \
            --verbose \
            -m "not remote_cluster" \
            -rA test_runner/batch_others -n4

      # TODO kb add pytest benchmarks

